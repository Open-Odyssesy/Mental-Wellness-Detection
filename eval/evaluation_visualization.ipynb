{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa4acb5",
   "metadata": {},
   "source": [
    "# Mental Wellness Detection Model Evaluation Visualization\n",
    "\n",
    "This notebook provides interactive visualizations for model evaluation results, including confusion matrices, performance metrics, and comparative analysis.\n",
    "\n",
    "**Features:**\n",
    "- Load and analyze evaluation reports from JSON files\n",
    "- Interactive confusion matrix heatmaps\n",
    "- Per-class performance metrics visualization\n",
    "- ROC curves and AUC analysis\n",
    "- Class distribution analysis\n",
    "- Report comparison across different evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34989342",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import necessary libraries for evaluation, visualization, and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea1ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369e551a",
   "metadata": {},
   "source": [
    "## 2. Load Evaluation Reports\n",
    "\n",
    "Load and analyze evaluation metrics from JSON reports in the reports directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evaluation_reports(reports_dir='../reports'):\n",
    "    \"\"\"\n",
    "    Load all evaluation reports from the reports directory.\n",
    "\n",
    "    Args:\n",
    "        reports_dir (str): Directory containing evaluation reports\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation reports keyed by filename\n",
    "    \"\"\"\n",
    "    reports_dir = Path(reports_dir)\n",
    "    if not reports_dir.exists():\n",
    "        print(f\"‚ùå Reports directory not found: {reports_dir}\")\n",
    "        return {}\n",
    "\n",
    "    # Find all JSON evaluation files\n",
    "    json_files = list(reports_dir.glob('*_metrics.json'))\n",
    "\n",
    "    if not json_files:\n",
    "        print(f\"‚ùå No evaluation reports found in {reports_dir}\")\n",
    "        return {}\n",
    "\n",
    "    reports = {}\n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                report_name = json_file.stem.replace('_metrics', '')\n",
    "                reports[report_name] = json.load(f)\n",
    "                print(f\"‚úÖ Loaded report: {report_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {json_file}: {e}\")\n",
    "\n",
    "    return reports\n",
    "\n",
    "# Load all evaluation reports\n",
    "evaluation_reports = load_evaluation_reports()\n",
    "\n",
    "if evaluation_reports:\n",
    "    print(f\"\\nüìä Found {len(evaluation_reports)} evaluation report(s)\")\n",
    "    for name in evaluation_reports.keys():\n",
    "        print(f\"   - {name}\")\n",
    "else:\n",
    "    print(\"‚ùå No evaluation reports found. Please run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c934a44",
   "metadata": {},
   "source": [
    "## 3. Evaluation Summary\n",
    "\n",
    "Display summary statistics for all loaded evaluation reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe2f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_evaluation_summary(reports):\n",
    "    \"\"\"\n",
    "    Display a summary table of all evaluation reports.\n",
    "\n",
    "    Args:\n",
    "        reports (dict): Dictionary of evaluation reports\n",
    "    \"\"\"\n",
    "    if not reports:\n",
    "        print(\"No reports to display\")\n",
    "        return\n",
    "\n",
    "    summary_data = []\n",
    "    for report_name, metrics in reports.items():\n",
    "        summary_data.append({\n",
    "            'Report': report_name,\n",
    "            'Accuracy': f\"{metrics['accuracy']:.4f}\",\n",
    "            'Precision': f\"{metrics['precision']:.4f}\",\n",
    "            'Recall': f\"{metrics['recall']:.4f}\",\n",
    "            'F1 Score': f\"{metrics['f1_score']:.4f}\",\n",
    "            'AUC Score': f\"{metrics.get('auc_score', 'N/A'):.4f}\" if metrics.get('auc_score') else 'N/A',\n",
    "            'Samples': metrics['num_samples'],\n",
    "            'Classes': metrics['num_classes']\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    display(HTML(summary_df.to_html(index=False, classes='table table-striped')))\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "# Display evaluation summary\n",
    "if evaluation_reports:\n",
    "    summary_df = display_evaluation_summary(evaluation_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7b72a",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix Visualization\n",
    "\n",
    "Interactive confusion matrix heatmap for selected evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0019bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(report_name, metrics):\n",
    "    \"\"\"\n",
    "    Plot interactive confusion matrix heatmap.\n",
    "\n",
    "    Args:\n",
    "        report_name (str): Name of the evaluation report\n",
    "        metrics (dict): Evaluation metrics dictionary\n",
    "    \"\"\"\n",
    "    conf_matrix = np.array(metrics['confusion_matrix'])\n",
    "    class_labels = list(metrics['class_distribution'].keys())\n",
    "\n",
    "    # Create heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=conf_matrix,\n",
    "        x=[f'Predicted {label}' for label in class_labels],\n",
    "        y=[f'Actual {label}' for label in class_labels],\n",
    "        colorscale='Blues',\n",
    "        text=conf_matrix,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 12},\n",
    "        hoverongaps=False\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Confusion Matrix - {report_name}',\n",
    "        xaxis_title=\"Predicted Label\",\n",
    "        yaxis_title=\"Actual Label\",\n",
    "        width=600,\n",
    "        height=500\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Interactive widget for selecting evaluation report\n",
    "if evaluation_reports:\n",
    "    report_selector = widgets.Dropdown(\n",
    "        options=list(evaluation_reports.keys()),\n",
    "        value=list(evaluation_reports.keys())[0],\n",
    "        description='Select Report:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    def on_report_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            clear_output(wait=True)\n",
    "            display(report_selector)\n",
    "            selected_report = change['new']\n",
    "            plot_confusion_matrix(selected_report, evaluation_reports[selected_report])\n",
    "\n",
    "    report_selector.observe(on_report_change)\n",
    "\n",
    "    display(report_selector)\n",
    "    # Display initial plot\n",
    "    initial_report = list(evaluation_reports.keys())[0]\n",
    "    plot_confusion_matrix(initial_report, evaluation_reports[initial_report])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f0812c",
   "metadata": {},
   "source": [
    "## 5. Per-Class Performance Metrics\n",
    "\n",
    "Visualize precision, recall, and F1-score for each class across all evaluation reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9135832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_per_class_metrics(reports):\n",
    "    \"\"\"\n",
    "    Plot per-class precision, recall, and F1-score for all reports.\n",
    "\n",
    "    Args:\n",
    "        reports (dict): Dictionary of evaluation reports\n",
    "    \"\"\"\n",
    "    if not reports:\n",
    "        return\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    for report_name, metrics in reports.items():\n",
    "        class_labels = list(metrics['class_distribution'].keys())\n",
    "        for i, class_label in enumerate(class_labels):\n",
    "            plot_data.append({\n",
    "                'Report': report_name,\n",
    "                'Class': f'Class {class_label}',\n",
    "                'Precision': metrics['precision_per_class'][i],\n",
    "                'Recall': metrics['recall_per_class'][i],\n",
    "                'F1 Score': metrics['f1_per_class'][i]\n",
    "            })\n",
    "\n",
    "    df_plot = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=('Precision by Class', 'Recall by Class', 'F1 Score by Class'),\n",
    "        shared_yaxes=True\n",
    "    )\n",
    "\n",
    "    # Precision plot\n",
    "    for report_name in df_plot['Report'].unique():\n",
    "        report_data = df_plot[df_plot['Report'] == report_name]\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                name=f'{report_name} - Precision',\n",
    "                x=report_data['Class'],\n",
    "                y=report_data['Precision'],\n",
    "                offsetgroup=0\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "    # Recall plot\n",
    "    for report_name in df_plot['Report'].unique():\n",
    "        report_data = df_plot[df_plot['Report'] == report_name]\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                name=f'{report_name} - Recall',\n",
    "                x=report_data['Class'],\n",
    "                y=report_data['Recall'],\n",
    "                offsetgroup=0\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "    # F1 Score plot\n",
    "    for report_name in df_plot['Report'].unique():\n",
    "        report_data = df_plot[df_plot['Report'] == report_name]\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                name=f'{report_name} - F1',\n",
    "                x=report_data['Class'],\n",
    "                y=report_data['F1 Score'],\n",
    "                offsetgroup=0\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Per-Class Performance Metrics Comparison',\n",
    "        height=500,\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Plot per-class metrics\n",
    "if evaluation_reports:\n",
    "    plot_per_class_metrics(evaluation_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c788ad",
   "metadata": {},
   "source": [
    "## 6. Class Distribution Analysis\n",
    "\n",
    "Visualize the distribution of classes in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(reports):\n",
    "    \"\"\"\n",
    "    Plot class distribution for all evaluation reports.\n",
    "\n",
    "    Args:\n",
    "        reports (dict): Dictionary of evaluation reports\n",
    "    \"\"\"\n",
    "    if not reports:\n",
    "        return\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    for report_name, metrics in reports.items():\n",
    "        for class_label, count in metrics['class_distribution'].items():\n",
    "            total_samples = sum(metrics['class_distribution'].values())\n",
    "            percentage = (count / total_samples) * 100\n",
    "            plot_data.append({\n",
    "                'Report': report_name,\n",
    "                'Class': f'Class {class_label}',\n",
    "                'Count': count,\n",
    "                'Percentage': percentage\n",
    "            })\n",
    "\n",
    "    df_plot = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Create pie chart for each report\n",
    "    reports_list = df_plot['Report'].unique()\n",
    "\n",
    "    if len(reports_list) == 1:\n",
    "        # Single report - simple pie chart\n",
    "        fig = px.pie(\n",
    "            df_plot,\n",
    "            values='Count',\n",
    "            names='Class',\n",
    "            title=f'Class Distribution - {reports_list[0]}',\n",
    "            hover_data=['Percentage']\n",
    "        )\n",
    "        fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "    else:\n",
    "        # Multiple reports - subplot\n",
    "        cols = 2\n",
    "        rows = (len(reports_list) + 1) // cols\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=rows, cols=cols,\n",
    "            subplot_titles=[f'Class Distribution - {report}' for report in reports_list],\n",
    "            specs=[[{'type': 'pie'} for _ in range(cols)] for _ in range(rows)]\n",
    "        )\n",
    "\n",
    "        for i, report_name in enumerate(reports_list):\n",
    "            report_data = df_plot[df_plot['Report'] == report_name]\n",
    "            row = i // cols + 1\n",
    "            col = i % cols + 1\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Pie(\n",
    "                    labels=report_data['Class'],\n",
    "                    values=report_data['Count'],\n",
    "                    name=report_name,\n",
    "                    hovertemplate='%{label}<br>Count: %{value}<br>Percentage: %{percent}'\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "\n",
    "    fig.update_layout(height=400 * ((len(reports_list) + 1) // 2))\n",
    "    fig.show()\n",
    "\n",
    "# Plot class distribution\n",
    "if evaluation_reports:\n",
    "    plot_class_distribution(evaluation_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d8ac5",
   "metadata": {},
   "source": [
    "## 7. Export Visualizations\n",
    "\n",
    "Save plots and visualizations to files for reports or presentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b20fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_visualizations(reports, output_dir='../reports/visualizations'):\n",
    "    \"\"\"\n",
    "    Export all visualizations to image files.\n",
    "\n",
    "    Args:\n",
    "        reports (dict): Dictionary of evaluation reports\n",
    "        output_dir (str): Directory to save visualizations\n",
    "    \"\"\"\n",
    "    import plotly.io as pio\n",
    "\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"üìä Exporting visualizations to {output_dir}\")\n",
    "\n",
    "    # Export confusion matrices\n",
    "    for report_name, metrics in reports.items():\n",
    "        # Confusion matrix plot\n",
    "        conf_matrix = np.array(metrics['confusion_matrix'])\n",
    "        class_labels = list(metrics['class_distribution'].keys())\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            conf_matrix,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=[f'Pred {label}' for label in class_labels],\n",
    "            yticklabels=[f'Actual {label}' for label in class_labels]\n",
    "        )\n",
    "        plt.title(f'Confusion Matrix - {report_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / f'{report_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Per-class metrics bar plot\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        metrics_names = ['Precision', 'Recall', 'F1 Score']\n",
    "        metrics_data = [\n",
    "            metrics['precision_per_class'],\n",
    "            metrics['recall_per_class'],\n",
    "            metrics['f1_per_class']\n",
    "        ]\n",
    "\n",
    "        for i, (name, data) in enumerate(zip(metrics_names, metrics_data)):\n",
    "            axes[i].bar([f'Class {label}' for label in class_labels], data)\n",
    "            axes[i].set_title(f'{name} by Class')\n",
    "            axes[i].set_ylim(0, 1)\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        plt.suptitle(f'Per-Class Metrics - {report_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / f'{report_name}_per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"‚úÖ Exported plots for {report_name}\")\n",
    "\n",
    "    print(f\"üéâ All visualizations exported to {output_dir}\")\n",
    "\n",
    "# Export button\n",
    "if evaluation_reports:\n",
    "    export_button = widgets.Button(\n",
    "        description='Export Visualizations',\n",
    "        button_style='success',\n",
    "        tooltip='Export all plots to PNG files'\n",
    "    )\n",
    "\n",
    "    def on_export_click(b):\n",
    "        export_visualizations(evaluation_reports)\n",
    "\n",
    "    export_button.on_click(on_export_click)\n",
    "    display(export_button)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä Mental Wellness Detection Evaluation Visualization Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nFeatures:\")\n",
    "print(\"‚úÖ Interactive confusion matrix heatmaps\")\n",
    "print(\"‚úÖ Per-class performance metrics comparison\")\n",
    "print(\"‚úÖ Class distribution analysis\")\n",
    "print(\"‚úÖ Export functionality for reports\")\n",
    "print(\"\\nReports loaded:\", len(evaluation_reports))\n",
    "print(\"Ready for analysis and insights!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
